"""
analyze_predictions.py - Ph√¢n t√≠ch chi ti·∫øt prediction accuracy v√† bias

Script n√†y ph√¢n t√≠ch predictions_log.json ƒë·ªÉ:
1. T√≠nh accuracy t·ªïng th·ªÉ v√† theo confidence level
2. Ph√°t hi·ªán bias (Over/Under win rate)
3. Calibration analysis (confidence vs actual accuracy)
4. Performance theo th·ªùi gian
"""

import json
import os
from datetime import datetime
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt

PREDICTIONS_FILE = 'predictions_log.json'


def load_predictions():
    """Load predictions log."""
    if not os.path.exists(PREDICTIONS_FILE):
        print(f'‚ùå Kh√¥ng t√¨m th·∫•y file: {PREDICTIONS_FILE}')
        return []
    
    with open(PREDICTIONS_FILE, 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    
    return predictions


def analyze_handicap_accuracy(predictions):
    """Ph√¢n t√≠ch accuracy cho k√®o ch·∫•p."""
    completed = [p for p in predictions if p.get('actual_result') is not None]
    
    if not completed:
        print('‚ö†Ô∏è Ch∆∞a c√≥ tr·∫≠n n√†o ho√†n th√†nh.')
        return
    
    total = len(completed)
    correct = sum(1 for p in completed if p.get('correct'))
    accuracy = correct / total
    
    print('='*60)
    print('üìä PH√ÇN T√çCH K√àO CH·∫§P CH√ÇU √Å')
    print('='*60)
    print(f'T·ªïng s·ªë d·ª± ƒëo√°n: {len(predictions)}')
    print(f'ƒê√£ ho√†n th√†nh: {total}')
    print(f'D·ª± ƒëo√°n ƒë√∫ng: {correct}')
    print(f'ƒê·ªô ch√≠nh x√°c: {accuracy:.2%}')
    print()
    
    # Accuracy by confidence level
    conf_buckets = {
        'Cao (‚â•70%)': [p for p in completed if p.get('confidence', 0) >= 0.7],
        'Trung (55-70%)': [p for p in completed if 0.55 <= p.get('confidence', 0) < 0.7],
        'Th·∫•p (<55%)': [p for p in completed if p.get('confidence', 0) < 0.55],
    }
    
    print('Theo ƒë·ªô tin c·∫≠y:')
    for label, preds in conf_buckets.items():
        if preds:
            correct_in_bucket = sum(1 for p in preds if p.get('correct'))
            acc_in_bucket = correct_in_bucket / len(preds)
            print(f'  {label}: {acc_in_bucket:.2%} ({correct_in_bucket}/{len(preds)})')
    print()
    
    # By home/away pick
    home_picks = [p for p in completed if p.get('prediction') == 1]
    away_picks = [p for p in completed if p.get('prediction') == 0]
    
    if home_picks:
        home_correct = sum(1 for p in home_picks if p.get('correct'))
        print(f'Ch·ªçn Nh√†: {home_correct}/{len(home_picks)} ƒë√∫ng ({home_correct/len(home_picks):.2%})')
    if away_picks:
        away_correct = sum(1 for p in away_picks if p.get('correct'))
        print(f'Ch·ªçn Kh√°ch: {away_correct}/{len(away_picks)} ƒë√∫ng ({away_correct/len(away_picks):.2%})')
    print()


def analyze_ou_bias(predictions):
    """Ph√¢n t√≠ch bias Over/Under."""
    completed = [p for p in predictions 
                 if p.get('ou_pick') and p.get('ou_actual') and p.get('ou_actual') != 'Push']
    
    if not completed:
        print('‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu O/U ho√†n th√†nh.')
        return
    
    print('='*60)
    print('üéØ PH√ÇN T√çCH OVER/UNDER BIAS')
    print('='*60)
    
    # Overall O/U accuracy
    total = len(completed)
    correct = sum(1 for p in completed if p.get('ou_correct'))
    accuracy = correct / total
    print(f'T·ªïng s·ªë d·ª± ƒëo√°n O/U: {total}')
    print(f'ƒê·ªô ch√≠nh x√°c: {accuracy:.2%} ({correct}/{total})')
    print()
    
    # Pick distribution
    over_picks = [p for p in completed if p.get('ou_pick') == 'Over']
    under_picks = [p for p in completed if p.get('ou_pick') == 'Under']
    
    print(f'Ph√¢n b·ªï pick:')
    print(f'  Over: {len(over_picks)} ({len(over_picks)/total:.1%})')
    print(f'  Under: {len(under_picks)} ({len(under_picks)/total:.1%})')
    print()
    
    # Win rate by pick
    if over_picks:
        over_correct = sum(1 for p in over_picks if p.get('ou_correct'))
        over_wr = over_correct / len(over_picks)
        print(f'Win rate khi pick Over: {over_wr:.2%} ({over_correct}/{len(over_picks)})')
    
    if under_picks:
        under_correct = sum(1 for p in under_picks if p.get('ou_correct'))
        under_wr = under_correct / len(under_picks)
        print(f'Win rate khi pick Under: {under_wr:.2%} ({under_correct}/{len(under_picks)})')
    print()
    
    # BIAS detection
    if over_picks and under_picks:
        over_bias = (len(over_picks) / total) - 0.5
        print(f'üìà Over Bias: {over_bias:+.1%} ({"Nghi√™ng Over" if over_bias > 0.1 else ("Nghi√™ng Under" if over_bias < -0.1 else "C√¢n b·∫±ng")})')
        
        # Performance vs market expectation
        # If we're picking Over too much, but win rate is low -> overconfident on Over
        if over_bias > 0.15 and over_wr < 0.5:
            print('‚ö†Ô∏è C·∫£nh b√°o: Model nghi√™ng Over qu√° m·ª©c nh∆∞ng win rate th·∫•p!')
        elif over_bias < -0.15 and under_wr < 0.5:
            print('‚ö†Ô∏è C·∫£nh b√°o: Model nghi√™ng Under qu√° m·ª©c nh∆∞ng win rate th·∫•p!')
    print()
    
    # By line
    lines = {}
    for p in completed:
        line = p.get('ou_line')
        if line:
            if line not in lines:
                lines[line] = []
            lines[line].append(p)
    
    if lines:
        print('Theo t·ª´ng line:')
        for line in sorted(lines.keys()):
            preds = lines[line]
            correct_at_line = sum(1 for p in preds if p.get('ou_correct'))
            acc_at_line = correct_at_line / len(preds)
            over_at_line = sum(1 for p in preds if p.get('ou_pick') == 'Over')
            print(f'  Line {line}: {acc_at_line:.2%} ({correct_at_line}/{len(preds)}) | Over picks: {over_at_line}/{len(preds)}')
    print()


def analyze_calibration(predictions):
    """Ph√¢n t√≠ch calibration: confidence c√≥ kh·ªõp v·ªõi accuracy th·ª±c t·∫ø kh√¥ng."""
    completed = [p for p in predictions if p.get('actual_result') is not None]
    
    if len(completed) < 10:
        print('‚ö†Ô∏è C·∫ßn √≠t nh·∫•t 10 tr·∫≠n ƒë·ªÉ ph√¢n t√≠ch calibration.')
        return
    
    print('='*60)
    print('üìê PH√ÇN T√çCH CALIBRATION (Confidence vs Accuracy)')
    print('='*60)
    
    # Group by confidence bins
    bins = [
        (0.5, 0.6, '50-60%'),
        (0.6, 0.7, '60-70%'),
        (0.7, 0.8, '70-80%'),
        (0.8, 0.9, '80-90%'),
        (0.9, 1.0, '90-100%'),
    ]
    
    print('Confidence Range | Actual Accuracy | Count | Calibration Gap')
    print('-'*60)
    
    for min_conf, max_conf, label in bins:
        in_bin = [p for p in completed if min_conf <= p.get('confidence', 0) < max_conf]
        if in_bin:
            actual_acc = sum(1 for p in in_bin if p.get('correct')) / len(in_bin)
            expected_conf = sum(p.get('confidence', 0) for p in in_bin) / len(in_bin)
            gap = actual_acc - expected_conf
            
            gap_str = f'{gap:+.1%}'
            if abs(gap) > 0.15:
                gap_str += ' ‚ö†Ô∏è (Poorly calibrated)'
            elif abs(gap) < 0.05:
                gap_str += ' ‚úÖ (Well calibrated)'
            
            print(f'{label:16s} | {actual_acc:15.1%} | {len(in_bin):5d} | {gap_str}')
    print()


def analyze_goals_prediction(predictions):
    """Ph√¢n t√≠ch ƒë·ªô ch√≠nh x√°c d·ª± ƒëo√°n t·ªïng b√†n."""
    completed = [p for p in predictions 
                 if p.get('predicted_goals') is not None 
                 and p.get('home_goals') is not None 
                 and p.get('away_goals') is not None]
    
    if not completed:
        print('‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ªïng b√†n ho√†n th√†nh.')
        return
    
    print('='*60)
    print('‚öΩ PH√ÇN T√çCH D·ª∞ ƒêO√ÅN T·ªîNG B√ÄN TH·∫ÆNG')
    print('='*60)
    
    errors = []
    for p in completed:
        predicted = p.get('predicted_goals', 0)
        actual = (p.get('home_goals', 0) or 0) + (p.get('away_goals', 0) or 0)
        error = abs(predicted - actual)
        errors.append(error)
    
    mae = sum(errors) / len(errors)
    print(f'S·ªë tr·∫≠n: {len(completed)}')
    print(f'MAE (Mean Absolute Error): {mae:.2f} b√†n')
    print(f'D·ª± ƒëo√°n trung b√¨nh: {sum(p.get("predicted_goals", 0) for p in completed) / len(completed):.2f} b√†n')
    print(f'T·ªïng b√†n th·ª±c t·∫ø trung b√¨nh: {sum((p.get("home_goals", 0) or 0) + (p.get("away_goals", 0) or 0) for p in completed) / len(completed):.2f} b√†n')
    
    # Bias
    over_predictions = sum(1 for p in completed 
                          if p.get('predicted_goals', 0) > ((p.get('home_goals', 0) or 0) + (p.get('away_goals', 0) or 0)))
    under_predictions = len(completed) - over_predictions
    print(f'\nD·ª± ƒëo√°n cao h∆°n th·ª±c t·∫ø: {over_predictions}/{len(completed)} ({over_predictions/len(completed):.1%})')
    print(f'D·ª± ƒëo√°n th·∫•p h∆°n th·ª±c t·∫ø: {under_predictions}/{len(completed)} ({under_predictions/len(completed):.1%})')
    print()


def print_recent_predictions(predictions, n=10):
    """In danh s√°ch n predictions g·∫ßn nh·∫•t."""
    recent = predictions[-n:] if len(predictions) > n else predictions
    
    print('='*60)
    print(f'üìù {min(n, len(recent))} D·ª∞ ƒêO√ÅN G·∫¶N NH·∫§T')
    print('='*60)
    
    for p in reversed(recent):
        status = '‚è≥'
        if p.get('actual_result') is not None:
            status = '‚úÖ' if p.get('correct') else '‚ùå'
        
        conf = p.get('confidence', 0)
        pick = 'Nh√†' if p.get('prediction') == 1 else 'Kh√°ch'
        
        date_str = ''
        if p.get('timestamp'):
            try:
                dt = datetime.fromisoformat(p['timestamp'])
                date_str = dt.strftime('%d/%m %H:%M')
            except:
                pass
        
        print(f'{status} {date_str:12s} {p["home_team"]:20s} vs {p["away_team"]:20s}')
        print(f'   Pick: {pick:5s} | Conf: {conf:.1%}', end='')
        
        if p.get('home_goals') is not None:
            print(f' | T·ªâ s·ªë: {p["home_goals"]}-{p["away_goals"]}', end='')
        
        if p.get('ou_pick'):
            ou_status = ''
            if p.get('ou_correct') is not None:
                ou_status = ' ‚úÖ' if p['ou_correct'] else (' ‚ùå' if p['ou_correct'] is False else ' üü°')
            print(f' | O/U: {p["ou_pick"]}{ou_status}', end='')
        
        print()
    print()


def main():
    """Ch·∫°y to√†n b·ªô ph√¢n t√≠ch."""
    predictions = load_predictions()
    
    if not predictions:
        print('Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch.')
        return
    
    print(f'\nüìä PH√ÇN T√çCH PREDICTION TRACKER')
    print(f'Th·ªùi gian: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
    print(f'T·ªïng s·ªë predictions: {len(predictions)}')
    print()
    
    # Run all analyses
    analyze_handicap_accuracy(predictions)
    analyze_ou_bias(predictions)
    analyze_calibration(predictions)
    analyze_goals_prediction(predictions)
    print_recent_predictions(predictions, n=15)
    
    # Summary recommendations
    completed = [p for p in predictions if p.get('actual_result') is not None]
    if len(completed) >= 20:
        print('='*60)
        print('üí° G·ª¢I √ù C·∫¢I THI·ªÜN')
        print('='*60)
        
        # Check Over bias
        ou_completed = [p for p in completed if p.get('ou_pick') and p.get('ou_actual')]
        if ou_completed:
            over_picks = sum(1 for p in ou_completed if p.get('ou_pick') == 'Over')
            over_ratio = over_picks / len(ou_completed)
            
            if over_ratio > 0.65:
                print('‚Ä¢ Model nghi√™ng Over qu√° m·ª©c (>65% picks l√† Over)')
                print('  ‚Üí G·ª£i √Ω: Gi·∫£m alpha trong calibration ho·∫∑c tƒÉng defensive dampening')
            elif over_ratio < 0.35:
                print('‚Ä¢ Model nghi√™ng Under qu√° m·ª©c (<35% picks l√† Over)')
                print('  ‚Üí G·ª£i √Ω: TƒÉng alpha ho·∫∑c ki·ªÉm tra scaler')
        
        # Check calibration
        high_conf = [p for p in completed if p.get('confidence', 0) >= 0.8]
        if len(high_conf) >= 5:
            high_conf_acc = sum(1 for p in high_conf if p.get('correct')) / len(high_conf)
            if high_conf_acc < 0.7:
                print('‚Ä¢ ƒê·ªô tin c·∫≠y cao (‚â•80%) nh∆∞ng accuracy th·∫•p (<70%)')
                print('  ‚Üí G·ª£i √Ω: Model overconfident, c·∫ßn recalibrate ho·∫∑c th√™m regularization')
        
        print()


if __name__ == '__main__':
    main()
