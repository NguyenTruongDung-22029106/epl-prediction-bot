"""
Test script Ä‘á»ƒ validate !analyze vÃ  !fetchresults commands
Sá»­ dá»¥ng mock data tá»« predictions_log.json
"""

import json
import os
from datetime import datetime, timedelta

def test_analyze_logic():
    """Test logic cá»§a !analyze command"""
    print("=" * 60)
    print("TEST 1: Analyze Command Logic")
    print("=" * 60)
    
    if not os.path.exists('predictions_log.json'):
        print("âŒ KhÃ´ng tÃ¬m tháº¥y predictions_log.json")
        return False
    
    with open('predictions_log.json', 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    
    completed = [p for p in predictions if p.get('actual_result') is not None]
    
    if not completed:
        print("âš ï¸ ChÆ°a cÃ³ tráº­n nÃ o hoÃ n thÃ nh")
        return False
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ predictions: {len(predictions)}")
    print(f"âœ… ÄÃ£ hoÃ n thÃ nh: {len(completed)}")
    print(f"â³ Äang chá»: {len(predictions) - len(completed)}")
    
    # Overall accuracy
    total = len(completed)
    correct = sum(1 for p in completed if p.get('correct'))
    accuracy = correct / total
    
    acc_icon = 'ğŸŸ¢' if accuracy >= 0.65 else ('ğŸŸ¡' if accuracy >= 0.55 else 'ğŸ”´')
    print(f"\n{acc_icon} Äá»™ ChÃ­nh XÃ¡c Tá»•ng Thá»ƒ: {accuracy:.1%} ({correct}/{total})")
    
    # By confidence level
    high_conf = [p for p in completed if p.get('confidence', 0) >= 0.7]
    med_conf = [p for p in completed if 0.55 <= p.get('confidence', 0) < 0.7]
    
    if high_conf:
        high_acc = sum(1 for p in high_conf if p.get('correct')) / len(high_conf)
        print(f"\nğŸ“ˆ Confidence Cao (â‰¥70%): {high_acc:.1%} ({len(high_conf)} tráº­n)")
    
    if med_conf:
        med_acc = sum(1 for p in med_conf if p.get('correct')) / len(med_conf)
        print(f"ğŸ“ˆ Confidence Trung (55-70%): {med_acc:.1%} ({len(med_conf)} tráº­n)")
    
    # O/U Analysis
    ou_completed = [p for p in completed if p.get('ou_pick') and p.get('ou_actual') and p.get('ou_actual') != 'Push']
    
    if ou_completed:
        print(f"\nğŸ¯ Over/Under Analysis ({len(ou_completed)} tráº­n):")
        
        ou_correct = sum(1 for p in ou_completed if p.get('ou_correct'))
        ou_accuracy = ou_correct / len(ou_completed)
        print(f"   Accuracy: {ou_accuracy:.1%} ({ou_correct}/{len(ou_completed)})")
        
        over_picks = sum(1 for p in ou_completed if p.get('ou_pick') == 'Over')
        over_ratio = over_picks / len(ou_completed)
        
        print(f"   Over picks: {over_picks} ({over_ratio:.1%})")
        print(f"   Under picks: {len(ou_completed) - over_picks} ({(1-over_ratio):.1%})")
        
        # Bias detection
        if over_ratio > 0.65:
            print(f"   âš ï¸ OVER BIAS DETECTED: {over_ratio:.1%} picks lÃ  Over")
        elif over_ratio < 0.35:
            print(f"   âš ï¸ UNDER BIAS DETECTED: {(1-over_ratio):.1%} picks lÃ  Under")
        else:
            print(f"   âœ… CÃ¢n báº±ng tá»‘t")
        
        # Win rate by pick
        over_preds = [p for p in ou_completed if p.get('ou_pick') == 'Over']
        under_preds = [p for p in ou_completed if p.get('ou_pick') == 'Under']
        
        if over_preds:
            over_wr = sum(1 for p in over_preds if p.get('ou_correct')) / len(over_preds)
            print(f"   Over Win Rate: {over_wr:.1%}")
        
        if under_preds:
            under_wr = sum(1 for p in under_preds if p.get('ou_correct')) / len(under_preds)
            print(f"   Under Win Rate: {under_wr:.1%}")
    
    # Goals prediction
    goals_completed = [p for p in completed 
                      if p.get('predicted_goals') is not None 
                      and p.get('home_goals') is not None]
    
    if goals_completed:
        errors = []
        for p in goals_completed:
            predicted = p.get('predicted_goals', 0)
            actual = (p.get('home_goals', 0) or 0) + (p.get('away_goals', 0) or 0)
            errors.append(abs(predicted - actual))
        
        mae = sum(errors) / len(errors)
        print(f"\nâš½ Dá»± ÄoÃ¡n Tá»•ng BÃ n:")
        print(f"   MAE: {mae:.2f} bÃ n/tráº­n ({len(goals_completed)} tráº­n)")
    
    # Recent results
    print(f"\nğŸ“ 5 Tráº­n Gáº§n Nháº¥t:")
    recent = completed[-5:] if len(completed) > 5 else completed
    for p in reversed(recent):
        icon = 'âœ…' if p.get('correct') else 'âŒ'
        score = f"{p.get('home_goals', '?')}-{p.get('away_goals', '?')}"
        teams = f"{p['home_team'][:12]} vs {p['away_team'][:12]}"
        print(f"   {icon} {teams:30} ({score})")
    
    print("\nâœ… Test analyze logic PASSED")
    return True


def test_fetch_results_requirements():
    """Test requirements cho fetch results"""
    print("\n" + "=" * 60)
    print("TEST 2: Fetch Results Requirements")
    print("=" * 60)
    
    # Check if API key exists
    from dotenv import load_dotenv
    load_dotenv()
    
    api_key = os.getenv('FOOTBALL_DATA_API_KEY')
    if not api_key:
        print("âš ï¸ FOOTBALL_DATA_API_KEY chÆ°a Ä‘Æ°á»£c cáº¥u hÃ¬nh")
        print("   â†’ Cáº§n thÃªm vÃ o .env hoáº·c Render environment variables")
    else:
        print(f"âœ… FOOTBALL_DATA_API_KEY: {api_key[:10]}..." if len(api_key) > 10 else "âœ… FOOTBALL_DATA_API_KEY configured")
    
    # Check pending predictions
    if not os.path.exists('predictions_log.json'):
        print("âŒ KhÃ´ng tÃ¬m tháº¥y predictions_log.json")
        return False
    
    with open('predictions_log.json', 'r', encoding='utf-8') as f:
        predictions = json.load(f)
    
    pending = [p for p in predictions if p.get('actual_result') is None]
    print(f"\nğŸ“Š Pending predictions: {len(pending)}")
    
    if pending:
        print("\nâ³ Predictions Ä‘ang chá» káº¿t quáº£:")
        for p in pending[:5]:  # Show first 5
            match = f"{p['home_team']} vs {p['away_team']}"
            timestamp = p.get('timestamp', 'N/A')
            print(f"   - {match:40} ({timestamp})")
    else:
        print("   (Táº¥t cáº£ predictions Ä‘Ã£ cÃ³ káº¿t quáº£)")
    
    # Test auto_fetch_results function exists
    try:
        from prediction_tracker import auto_fetch_results
        print("\nâœ… auto_fetch_results function imported successfully")
        
        # Check function signature
        import inspect
        sig = inspect.signature(auto_fetch_results)
        print(f"   Signature: {sig}")
        
    except ImportError as e:
        print(f"\nâŒ Cannot import auto_fetch_results: {e}")
        return False
    
    print("\nâœ… Test fetch results requirements PASSED")
    return True


def test_integration_workflow():
    """Test toÃ n bá»™ workflow"""
    print("\n" + "=" * 60)
    print("TEST 3: Integration Workflow")
    print("=" * 60)
    
    # Simulate workflow
    print("\nğŸ“‹ Workflow:")
    print("   1. Bot táº¡o prediction â†’ âœ… (cÃ³ trong predictions_log.json)")
    print("   2. User cháº¡y !fetchresults â†’ â³ (cáº§n test vá»›i real API)")
    print("   3. User cháº¡y !analyze â†’ âœ… (logic tested above)")
    print("   4. Äiá»u chá»‰nh calibration náº¿u cáº§n â†’ â³ (manual)")
    
    # Check bot.py has new commands
    with open('bot.py', 'r', encoding='utf-8') as f:
        bot_content = f.read()
    
    has_analyze = '@bot.command(name=\'analyze\')' in bot_content
    has_fetchresults = '@bot.command(name=\'fetchresults\')' in bot_content
    
    print(f"\nğŸ¤– Bot Commands:")
    print(f"   !analyze: {'âœ…' if has_analyze else 'âŒ'}")
    print(f"   !fetchresults: {'âœ…' if has_fetchresults else 'âŒ'}")
    
    if has_analyze and has_fetchresults:
        print("\nâœ… Test integration workflow PASSED")
        return True
    else:
        print("\nâŒ Test integration workflow FAILED")
        return False


def main():
    print("\nğŸ§ª TESTING NEW COMMANDS: !analyze & !fetchresults")
    print("=" * 60)
    
    results = []
    
    # Run tests
    results.append(("Analyze Logic", test_analyze_logic()))
    results.append(("Fetch Requirements", test_fetch_results_requirements()))
    results.append(("Integration Workflow", test_integration_workflow()))
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š TEST SUMMARY")
    print("=" * 60)
    
    for test_name, passed in results:
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"{test_name:25} {status}")
    
    total = len(results)
    passed = sum(1 for _, p in results if p)
    
    print(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ ALL TESTS PASSED!")
        print("\nğŸ“ Next Steps:")
        print("   1. Deploy bot lÃªn Render (náº¿u chÆ°a)")
        print("   2. Test !analyze trong Discord vá»›i mock data")
        print("   3. Cháº¡y !fetchresults Ä‘á»ƒ fetch real data")
        print("   4. Monitor bias vá»›i !analyze sau má»—i gameweek")
    else:
        print(f"\nâš ï¸ {total - passed} test(s) failed. Review output above.")
    
    return passed == total


if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
